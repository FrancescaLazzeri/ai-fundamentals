{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.5.3-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Computer Vision\n",
        "Computer Vision is a branch of artificial intelligence (AI) that explores the development of AI systems that can \"see\" the world, either in real-time through a camera or by analyzing images and video.\n",
        "\n",
        "## Using the Computer Vision Cognitive Service\n",
        "\n",
        "Microsoft Azure includes a number of *cognitive services* that encapsulate common AI functions, including some that can help you build computer vision solutions.\n",
        "\n",
        "The *Computer Vision* cognitive service provides an obvious starting point for our exploration of computer vision in Azure. It uses pre-trained machine learning models to analyze images and extract information about them.\n",
        "\n",
        "For example, suppose Adventure Works Cycles has set up a number of cameras around the city to track the cycles that have been rented. By using the Computer Vision service, the images taken by the cameras can be analyzed to provide meaningful descriptions of what they depict.\n",
        "\n",
        "> **Citation**: The images used in this lab are from the [PASCAL Visual Object Classes (VOC) challenge dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).\n",
        "\n",
        "Let's start by creating a **Cognitive Services** resource in your Azure subscription:\n",
        "\n",
        "1. Open the [Azure portal](https://portal.azure.com), signing in with your Microsoft account.\n",
        "2. Click the **&#65291;Create a resource** button, search for *Cognitive Services*, and create a **Cognitive Services** resource with the following settings:\n",
        "    - **Name**: *Enter a unique name*.\n",
        "    - **Subscription**: *Your Azure subscription*.\n",
        "    - **Location**: *Any available location*.\n",
        "    - **Pricing tier**: S0\n",
        "    - **Resource group**: *Create a resource group with a unique name*.\n",
        "3. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Quick start** page, note the keys and endpoint. You will need these to connect to your cognitive services resource from client applications.\n",
        "4. Copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
        "5. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
        "6. Run the code in the cell below by clicking its green <span style=\"color:green\">&#9655</span> button.\n",
        "\n",
        "> **Note**: If you restart this notebook, you may need to run this cell again to reinitialize these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Now that you've set up the key and endpoint, you can use the custom vision service to analyze an image.\n",
        "\n",
        "Run the following cell to get a description for an image in the */data/voc/2009_004642.jpg* file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get a client for the computer vision service\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Open an image and display it\n",
        "image_path = os.path.join('data', 'voc', '2009_004642.jpg')\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "\n",
        "# Get a description from the computer vision service\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description_results = computervision_client.describe_image_in_stream(image_stream )\n",
        "if (len(description_results.captions) == 0):\n",
        "    print(\"No description detected.\")\n",
        "else:\n",
        "    for caption in description_results.captions:\n",
        "        print(\"'{}' with confidence {:.2f}%\".format(caption.text, caption.confidence * 100))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The description provided seems to be pretty accurate.\n",
        "\n",
        "The Computer Vision cognitive service offers a lot more functionality than generating image descriptions, including:\n",
        "\n",
        "- Suggesting \"tags\" for images, that can be useful if you want to index a lot of images for searching.\n",
        "- Identifying celebrities or well-known landmarks in images.\n",
        "- Detecting brand logos in an image.\n",
        "- Performing optical character recognition (OCR) to read text in an image.\n",
        "- Detect adult content in an image.\n",
        "\n",
        "> **Learn More**: To learn more about the Computer Vision cognitive service, see the [Computer Vision documentation](https://docs.microsoft.com/azure/cognitive-services/computer-vision/)\n",
        "\n",
        "## Using the Custom Vision Cognitive service\n",
        "\n",
        "The Computer Vision cognitive service provides useful pre-built models for working with images, but you'll often need to train your own model for computer vision. For example, suppose Adventure Works Cycles wants to use the cameras around the city to analyze traffic by identifying images of cars, buses, and cyclists. To do this, you'll need to train a *classification* model that can categorize images into these classes of road user.\n",
        "\n",
        "### Create a Custom Vision Authoring Resource\n",
        "\n",
        "The *Custom Vision* service enables you to train image classification and object detection models using your own set of training images, so you can create a model that is tailored for your needs. There are two parts to a custom vision solution - first you must *train* a model using your image data, and then you can *predict* image classes using your trained model. You can use your existing Cognitive Services resource to generate *predictions*, but you must create a separate resource for *training* the model.\n",
        "\n",
        "1. In another browser tab, open the Azure portal (<a href='https://portal.azure.com' target='_blank'>https://portal.azure.com</a>), signing in with your Microsoft account.\n",
        "2. Click **+ Create a resource**, and search for *Custom Vision*.\n",
        "3. In the list of services, click **Custom Vision**.\n",
        "4. In the **Custom Vision** blade, click **Create**.\n",
        "5. In the **Create** blade, select only **Training** (you will use your existing cognitive services resource for prediction). Then enter the following details and click **Create**\n",
        "  * **Name**: A unique name for your service\n",
        "  * **Subscription**: Your Azure subscription\n",
        "  * **Resource Group**: The existing resource group you used previously\n",
        "  * **Training location**: The same location as your cognitive services resource\n",
        "  * **Training pricing tier**: F0\n",
        "6. Wait for the service to be created.\n",
        "\n",
        "### Train an Image Classification Model\n",
        "Now you can use the *Custom Vision* cognitive service to train your own computer vision model based on existing images.\n",
        "\n",
        "1. Download and extract the training images from **[here](https://github.com/GraemeMalcolm/ai-fundamentals/raw/master/data/voc/training_images.zip)**.\n",
        "2. Open the **[Custom Vision portal](https://www.customvision.ai/projects)**. If prompted, sign in using the Microsoft account associated with your Azure subscription.\n",
        "3. In the Custom Vision portal, create a new project with the following settings:\n",
        "    - **Name**: Traffic Classification\n",
        "    - **Description**: Image classification for traffic.\n",
        "    - **Resource**: *The custom vision resource you created previously in this lab*\n",
        "    - **Project Types**: Classification\n",
        "    - **Classification Types**: Multiclass (single tag per image)\n",
        "    - **Domains**: General\n",
        "4. Click **\\[+\\] Add images**, and select all of the files in the **bus** folder you extracted previously. Then upload the image files, specifying the tag *bus*.\n",
        "5. Repeat the previous step to upload the images in the **car** folder with the tag *car*, and the images in the **cyclist** folder with the tag *cyclist*.\n",
        "6. Explore the images you have uploaded in the Custom Vision project - there should be 40 images of each class.\n",
        "7. In the Custom Vision project, click **Train** to train a classification model using the tagged images. Select the **Quick Training** option.\n",
        "8. Wait for training to complete, and then review the *Precision*, *Recall*, and **AP* performance metrics - these measure the prediction accuracy of the classification model, and should all be high.\n",
        "9. Click **&#128504; Publish** to publish the trained model with the following settings:\n",
        "    - **Model name**: traffic\n",
        "    - **Prediction Resource**: *Your cognitive services resource*.\n",
        "10. After publishing, click the **&#9881;** icon at the top right to view the project settings, and note the **Project Id**. Copy this value and paste it in the code cell below, replacing **YOUR_PROJECT_ID**; then run the code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_id = 'YOUR_PROJECT_ID'\n",
        "model_name = 'traffic'\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Now that you have trained and published your custom vision classification model, you can use it from a client application.\n",
        "\n",
        "Run the following code cell, which uses your model to classifiy a selection of test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the test images from the data/voc/test folder\n",
        "test_folder = os.path.join('data', 'voc', 'test')\n",
        "test_images = os.listdir(test_folder)\n",
        "\n",
        "# Create an instance of the prediction service\n",
        "custom_vision_client = CustomVisionPredictionClient(cog_key, endpoint=cog_endpoint)\n",
        "\n",
        "# Create a figure to display the results\n",
        "fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Get the images and show the predicted classes\n",
        "for idx in range(len(test_images)):\n",
        "    # Open the image, and use the custom vision model to classify it\n",
        "    image_contents = open(os.path.join(test_folder, test_images[idx]), \"rb\")\n",
        "    classification = custom_vision_client.classify_image(project_id, model_name, image_contents.read())\n",
        "    # The results include a prediction for each tag, in descending order of probability - get the first one\n",
        "    prediction = classification.predictions[0].tag_name\n",
        "    # Display the image with its predicted class\n",
        "    img = Image.open(os.path.join(test_folder, test_images[idx]))\n",
        "    a=fig.add_subplot(len(test_images)/3, 3,idx+1)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "    a.set_title(prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The Custom Vision cognitive service can classify new images based on the tags you specified in the training data. You can also use the Custom Vision service to create *object detection* models, which not only classify objects in images, but also identify *bounding boxes* that show the location of the object in the image.\n",
        "\n",
        "> **Learn More**: To learn more about the Custom Vision cognitive service, view the [Custom Vision documentation](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/home).\n",
        "\n",
        "## Using the Face Cognitive service\n",
        "\n",
        "You may have noticed that the Custom Vision model you trained actually identifies *cycles* rather than *cyclists*. It might be useful to extend the traffic analysis application to analyze images that are classified as *cyclist* to determine if they contain any human faces; and if so count the number of faces detected and highlight them in the image.\n",
        "\n",
        "To accomplish this, you'll use a third cognitive service that provides face detection and facial recognition capabilies.\n",
        "\n",
        "The code below performs the same image classification as previously, but now when a *cyclist* image is found, the code uses the **Face** cognitive service to detect faces in the image.\n",
        "\n",
        "Run the code cell below to see the results of this enhancement to the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the test images from the data/voc/test folder\n",
        "test_folder = os.path.join('data', 'voc', 'test')\n",
        "test_images = os.listdir(test_folder)\n",
        "\n",
        "# Create a prediction client\n",
        "custom_vision_client = CustomVisionPredictionClient(cog_key, endpoint=cog_endpoint)\n",
        "\n",
        "# Create a face detection client.\n",
        "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Create a figure to display the results\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "\n",
        "# Get the images and show the predicted classes\n",
        "for idx in range(len(test_images)):\n",
        "    # Open the image, and use the custom vision model to classify it\n",
        "    image_contents = open(os.path.join(test_folder, test_images[idx]), \"rb\")\n",
        "    classification = custom_vision_client.classify_image(project_id, model_name, image_contents.read())\n",
        "    # The results include a prediction for each tag, in descending order of probability - get the first one\n",
        "    prediction = classification.predictions[0].tag_name\n",
        "    # Open the image so we can add it to the figure\n",
        "    img = Image.open(os.path.join(test_folder, test_images[idx]))\n",
        "\n",
        "    # If the image is a cyclist, detect faces\n",
        "    if prediction == 'cyclist':\n",
        "        image_stream = open(os.path.join(test_folder, test_images[idx]), \"rb\")\n",
        "        detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "        if detected_faces:\n",
        "            # If there are faces, how many?\n",
        "            num_faces = len(detected_faces)\n",
        "            prediction = prediction + ' (' + str(num_faces) + ' faces detected)'\n",
        "            # Draw a rectangle around each detected face\n",
        "            for face in detected_faces:\n",
        "                r = face.face_rectangle\n",
        "                bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
        "                draw = ImageDraw.Draw(img)\n",
        "                draw.rectangle(bounding_box, outline='magenta', width=5)\n",
        "\n",
        "    # Display the image with its predicted class and detected faces\n",
        "    a=fig.add_subplot(len(test_images)/3, 3,idx+1)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "    a.set_title(prediction)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The Face cognitive service can do much more than simply detect faces. It can also analyze facial features and expressions to suggest gender, age, and emotional state; and it can compare faces for similarity and be trained to recognize individual faces.\n",
        "\n",
        "> **Learn More**: To learn more about the Face cognitive service, see the [Face documentation](https://docs.microsoft.com/azure/cognitive-services/face/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Optical Character Recognition to Read text in Images\n",
        "\n",
        "So far you've seen how you can use congitive services in Azure to analyze and classifiy images, and identify faces in photographs. Another common computer vision challenge is to detect and interpret text in an image. This kind of processing is often referred to as *optical character recognition* (OCR).\n",
        "\n",
        "### Read Text in an Image\n",
        "Let's start with a simple example. The **Computer Vision** cognitive service has some built-in OCR capabilities that enable you to detect the location of text in an image. Let's put it to the test to see if the Adventure Works street cameras can read text on road signs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get a client for the computer vision service\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Create a figure to display the results\n",
        "image_path = os.path.join('data', 'sign.jpg')\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "image_stream = open(image_path, \"rb\")\n",
        "ocr_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "for region in ocr_results.regions:\n",
        "    for line in region.lines:\n",
        "        for word in line.words:\n",
        "            print(word.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract Information from Forms\n",
        "\n",
        "A more advanced OCR scenario is the extraction of information from forms or invoices. The **Form Recognizer** service is specifically designed for this kind of AI problem.\n",
        "\n",
        "In this example, you'll use the Form Recognizer's built-in model for analyzing receipts.\n",
        "\n",
        "Start by creating a Form Recognizer resource in your Azure subscription:\n",
        "\n",
        "1. In another browser tab, open the Azure portal (<a href='https://portal.azure.com' target='_blank'>https://portal.azure.com</a>), signing in with your Microsoft account.\n",
        "2. Select **+ Create a resource**, and search for *Form Recognizer*.\n",
        "3. In the list of services, select **Form Recognizer**.\n",
        "4. In the **Form Recognizer** blade, select **Create**.\n",
        "5. In the **Create** blade, enter the following details and select **Create**\n",
        "  * **Name**: A unique name for your service\n",
        "  * **Subscription**: Your Azure subscription\n",
        "  * **Location**: Any available location\n",
        "  * **Pricing tier**: F0\n",
        "  * **Resource Group**: The existing resource group you used previously\n",
        "  * **I confirm I have read and understood the notice below**: Selected.\n",
        "6. Wait for the service to be created.\n",
        "7. View your newly created Form Recognizer service in the Azure portal and on the **Quick Start** page, copy the **Key1** and **Endpoint** values and paste them in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "form_key = '827d9c9bbf3a48cc99f66301ddab5d1c'\n",
        "form_endpoint = 'https://gmalc-form.cognitiveservices.azure.com/'\n",
        "print('Ready to use form recognizer at {} using key {}'.format(form_endpoint, form_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you're ready to use Form Recognizer to analyze a receipt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from requests import get, post\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# Create a figure to display the results\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "image_path = os.path.join('data', 'receipt.jpg')\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "\n",
        "post_url = form_endpoint + \"/formrecognizer/v2.0-preview/prebuilt/receipt/analyze\"\n",
        "\n",
        "headers = {\n",
        "    'Content-Type': 'image/jpeg',\n",
        "    'Ocp-Apim-Subscription-Key': form_key,\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"includeTextDetails\": True\n",
        "}\n",
        "\n",
        "with open(image_path, \"rb\") as f:\n",
        "    data_bytes = f.read()\n",
        "\n",
        "try:\n",
        "    resp = post(url = post_url, data = data_bytes, headers = headers, params = params)\n",
        "    if resp.status_code != 202:\n",
        "        print(\"POST analyze failed:\\n%s\" % resp.text)\n",
        "        quit()\n",
        "    print(\"POST request succeeded. Awaiting analysis results...\")\n",
        "    get_url = resp.headers[\"operation-location\"]\n",
        "except Exception as e:\n",
        "    print(\"POST analyze failed:\\n%s\" % str(e))\n",
        "    quit()\n",
        "\n",
        "n_tries = 10\n",
        "n_try = 0\n",
        "wait_sec = 6\n",
        "complete = False\n",
        "while n_try < n_tries and complete != True:\n",
        "    try:\n",
        "        resp = get(url = get_url, headers = {\"Ocp-Apim-Subscription-Key\": form_key})\n",
        "        resp_json = json.loads(resp.text)\n",
        "        if resp.status_code != 200:\n",
        "            print(\"GET Layout results failed:\\n%s\" % resp_json)\n",
        "            complete = True\n",
        "        status = resp_json[\"status\"]\n",
        "        if status == \"succeeded\":\n",
        "            fields = resp_json[\"analyzeResult\"][\"documentResults\"][0][\"fields\"]\n",
        "            for field in fields:\n",
        "                fieldType = fields[field][\"type\"]\n",
        "                if fieldType != 'array':\n",
        "                    fieldTypeString = fieldType[0].capitalize() + fieldType[1:]\n",
        "                    valueField = 'value' + fieldTypeString\n",
        "                    print(field, fields[field][valueField])\n",
        "            complete = True\n",
        "        if status == \"failed\":\n",
        "            print(\"Analysis failed:\\n%s\" % resp_json)\n",
        "            complete = True\n",
        "        # Analysis still running. Wait and retry.\n",
        "        time.sleep(wait_sec)\n",
        "        n_try += 1     \n",
        "    except Exception as e:\n",
        "        msg = \"GET analyze results failed:\\n%s\" % str(e)\n",
        "        print(msg)\n",
        "        complete = True"
      ]
    }
  ]
}